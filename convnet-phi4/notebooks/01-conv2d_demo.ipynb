{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8838a0-4dce-4804-9048-dabee2d835ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Two-dimensional Convolution Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfada8c2-ef2c-4036-9811-54596893e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "PI = math.pi\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False, threshold=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5121fe-94e9-418c-aa62-89838c4a07e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definition of convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2852bf-da6d-4c14-a50f-7493d26979a6",
   "metadata": {},
   "source": [
    "Let $\\phi(x)$ and $w(x)$ be real-valued scalar functions defined on $\\mathbb{R}^2$ which decay to zero as $x \\to \\pm \\infty$.\n",
    "Their convolution is another function defined by the integral transform,\n",
    "\n",
    "$$\n",
    "(w \\ast \\phi)(x) = \\int_{\\mathbb{R}^2} \\mathrm{d} y \\, w(y) \\phi(x - y) \\, .\n",
    "$$\n",
    "\n",
    "We will take the view that $\\phi(x)$ is the function being transformed, and $w(y)$ is the **kernel** of the transform.\n",
    "The discretized version of this operation is\n",
    "\n",
    "$$\n",
    "(w \\ast \\phi)(n) = \\sum_{m \\in \\mathbb{Z}^2} w(m) \\phi(n - m) \\, .\n",
    "$$\n",
    "\n",
    "We are interested in the situation where the domain is a square lattice $\\Lambda \\subset a \\mathbb{N}^2$, which is periodic in both dimensions with period $aL$, where $a$ is the lattice spacing.\n",
    "On this domain the convolution can be written\n",
    "\n",
    "$$\n",
    "    (w \\ast \\phi)(an) = \\sum_{m_1, m_2=0}^{L-1} w(am) \\phi\\big(a(n-m)_{\\text{mod} L}\\big) \\, .\n",
    "$$\n",
    "\n",
    "Let us set the lattice spacing to $a = 1$ so that $x \\equiv n$ and $y \\equiv m$.\n",
    "\n",
    "$$\n",
    "    (w \\ast \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x-y)_{\\text{mod} L}\\big) \\, .\n",
    "$$\n",
    "\n",
    "In fact, the traditional conventional convolutional neural network performs a cross-correlation '$\\star$' rather than a convolution '$\\ast$'.\n",
    "For real-valued functions this differs by nothing more than a reflection in $\\phi$;\n",
    "\n",
    "$$\n",
    "    (w \\star \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x+y)_{\\text{mod} L}\\big) \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644020f2-e3b5-46ff-a0cc-0f3c93f076bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorch implementation: `conv2d`\n",
    "\n",
    "### Kernel shape\n",
    "\n",
    "PyTorch implements two-dimensional cross-correlation via `torch.nn.functional.conv2d`. However, it expects the kernel to be a matrix centered on the (0, 0) element, and automatically pads this kernel with zeros to match the size of $\\phi$.\n",
    "\n",
    "For example, a nearest-neighbour kernel would look like the following\n",
    "\\begin{align}\n",
    "w =\n",
    "\\begin{pmatrix}\n",
    "0 & e & 0 \\\\\n",
    "d & a & b \\\\\n",
    "0 & c & 0\n",
    "\\end{pmatrix} \\, ,\n",
    "\\end{align}\n",
    "\n",
    "and not\n",
    "\n",
    "\\begin{align}\n",
    "w = \n",
    "\\begin{pmatrix}\n",
    "a & b & 0 & \\ldots & 0 & d \\\\\n",
    "c & 0 & 0 & \\ldots & 0 & 0 \\\\\n",
    "0 & 0 & 0 & & & \\vdots \\\\\n",
    "\\vdots & \\vdots & & \\ddots & & \\vdots \\\\\n",
    "0 & 0 & & & \\ddots & \\vdots\\\\\n",
    "e & 0 & \\ldots & \\ldots & \\ldots & 0\n",
    "\\end{pmatrix} \\, ,\n",
    "\\end{align}\n",
    "\n",
    "the latter being implied by the sum $\\sum_{y_1, y_2=0}^L w(y)$ in the equation above.\n",
    "\n",
    "### Input dimensions\n",
    "\n",
    "`conv2d` also expects both $w$ and $\\phi$ to have four dimensions:\n",
    "1. Batch dimension\n",
    "2. Channel dimension\n",
    "3. Width\n",
    "4. Height\n",
    "\n",
    "### Input padding\n",
    "\n",
    "By default, the kernel does not wrap around the lattice dimensions as we would like it to. We have to manually pad $\\phi$ in order to achieve this. For example, for a $3 \\times 3$ kernel we would have to pad each boundary with one extra lattice site:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "\\phi_{00} & \\ldots & \\phi_{0L} \\\\\n",
    "\\vdots & \\ddots  & \\vdots \\\\\n",
    "\\phi_{L0} & \\ldots & \\phi_{LL}\n",
    "\\end{pmatrix}\n",
    "\\longrightarrow\n",
    "\\begin{pmatrix}\n",
    "\\phi_{LL} & \\phi_{L0} & \\ldots & \\phi_{LL} & \\phi_{0L} \\\\\n",
    "\\phi_{0L} & \\phi_{00} & \\ldots & \\phi_{0L} & \\phi_{00} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\phi_{LL} & \\phi_{L0} & \\ldots & \\phi_{LL} & \\phi_{L0} \\\\\n",
    "\\phi_{0L} & \\phi_{00} & \\ldots & \\phi_{0L} & \\phi_{00}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc314bb-0567-4576-8797-33d15c85749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_inputs(phi, w):\n",
    "    \"\"\"Reshape and pad inputs for conv2d.\n",
    "\n",
    "    conv2d expects shapes:\n",
    "\n",
    "    - input (phi) : (batch size, input channels, width, height)\n",
    "    - kernel (w)  : (batch size, output channels, kernel width, kernel height)\n",
    "\n",
    "    However, our input is periodic so we also need to pad phi by an amount\n",
    "    K, which is the kernel radius.\n",
    "    \"\"\"\n",
    "    assert phi.dim() in (2, 3)\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    if phi.dim() == 2:\n",
    "        phi = phi.unsqueeze(0)  # add batch dim\n",
    "    phi = phi.unsqueeze(1)  # add channel dim\n",
    "\n",
    "    k1, k2 = [(k - 1) // 2 for k in w.shape]\n",
    "    w = w.view(1, 1, *w.shape)\n",
    "    phi = torch.nn.functional.pad(\n",
    "        phi,  # (n_batch, n_channels, width, height)\n",
    "        pad=(k2, k2, k1, k1),  # pad last 2 dimensions by kernel radius on each side\n",
    "        mode=\"circular\",\n",
    "    )\n",
    "    return phi, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d89fbc-25a5-4059-aac2-781f1043a441",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example: Identity transformation\n",
    "\n",
    "\\begin{align}\n",
    "w_{id} =\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37ad4e5-fd29-4e68-a3c8-51b99866266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 6\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "assert torch.allclose(conv_out, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff389b68-890b-4257-836e-bfdc261e7ada",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example: Shift one lattice unit to the left\n",
    "\n",
    "\\begin{align}\n",
    "w_{\\leftarrow} =\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada9f3e2-14f9-4c54-beec-6f24710c1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "phi = torch.arange(L * L).reshape(L, L).float()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "# Check the direction of torch.roll\n",
    "assert phi.roll(shifts=(-1, -1), dims=(0, 1))[0, 0] == phi[1, 1]\n",
    "\n",
    "assert torch.allclose(conv_out, phi.roll(-1, dims=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06ea89-2536-4bf5-8e98-49ff9da6ab01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `conv2d` implements cross-correlation\n",
    "\n",
    "We can check that conv2d implements cross-correlation, defined by\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\star \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x+y)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "This requires us to modify the kernel so that it looks like a full $L \\times L$ matrix with the `(0, 0)` element in the top-left corner.\n",
    "    \n",
    "For example, conv2d interprets the following tensor as having `b` in the `(0, 0)` position, `a` in the `(L, L)` position and `c` in the `(1, 1)` position:\n",
    "    \n",
    "```python\n",
    "torch.Tensor([\n",
    "    [a, 0, 0],\n",
    "    [0, b, 0],\n",
    "    [0, 0, c]\n",
    "])\n",
    "```\n",
    "However, to match our cross-correlation calculation we require an L1 x L2 tensor that looks like the following:\n",
    "    \n",
    "```python\n",
    "torch.Tensor([\n",
    "    [b, 0, ..., 0],\n",
    "    [0, c, ..., 0],\n",
    "         ...\n",
    "    [0, 0, ..., a]\n",
    "])\n",
    "```\n",
    "    \n",
    "Therefore, we need to:\n",
    "1. Pad the tensor with zeros to make it the same shape as the input\n",
    "2. Roll the kernel so that the (0, 0) element is in the top-left corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813574f7-3429-4176-be71-4b70a4263dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_roll_kernel(phi, w):\n",
    "    \"\"\"Pad and roll kernel so that we can compute the cross-correlation.\"\"\"\n",
    "    L1, L2 = phi.shape[-2:]\n",
    "    k1, k2 = [(k - 1) // 2 for k in w.shape]\n",
    "\n",
    "    # Pad with zeros to make it the same size as phi\n",
    "    w = torch.nn.functional.pad(\n",
    "        w, (0, L2 - w.shape[1], 0, L1 - w.shape[0]), mode=\"constant\", value=0\n",
    "    )\n",
    "\n",
    "    # Roll the kernel\n",
    "    w = w.roll((-k1, -k2), (0, 1))\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# ------------------------------------ #\n",
    "# TEST 1: expansion of diagonal kernel #\n",
    "# ------------------------------------ #\n",
    "L = 6\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [0, 2, 0],\n",
    "        [0, 0, 3],\n",
    "    ]\n",
    ").int()\n",
    "expected = torch.Tensor(\n",
    "    [\n",
    "        [2, 0, 0, 0, 0, 0],\n",
    "        [0, 3, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "    ]\n",
    ").int()\n",
    "result = pad_and_roll_kernel(phi, w)\n",
    "assert torch.equal(result, expected)\n",
    "\n",
    "\n",
    "# ----------------------------------- #\n",
    "# TEST 2: conv2d is cross correlation #\n",
    "# ----------------------------------- #\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "w = pad_and_roll_kernel(phi, w)\n",
    "cross_corr = torch.stack(\n",
    "    [\n",
    "        w[y1, y2] * phi.roll((-y1, -y2), (0, 1))\n",
    "        for y1, y2 in itertools.product(range(L), range(L))\n",
    "    ],\n",
    "    dim=0,\n",
    ").sum(dim=0)\n",
    "assert torch.allclose(cross_corr, conv_out)\n",
    "\n",
    "\n",
    "# ------------------------------------------------- #\n",
    "# TEST 3: explicit calculation of cross correlation #\n",
    "# ------------------------------------------------- #\n",
    "cross_corr_explicit = torch.zeros_like(phi)\n",
    "for x1 in range(L):\n",
    "    for x2 in range(L):\n",
    "        for y1 in range(L):\n",
    "            for y2 in range(L):\n",
    "                res = w[y1, y2] * phi[(x1 + y1) % L, (x2 + y2) % L]\n",
    "                cross_corr_explicit[x1, x2] += res\n",
    "assert torch.allclose(cross_corr_explicit, conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719bcf4-f261-426b-8d79-bcec7fb6a217",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross-correlation as a matrix-vector product\n",
    "\n",
    "Discrete cross correlation (or convolution) can be expressed as a matrix-vector product\n",
    "\n",
    "\\begin{align}\n",
    "w \\star \\phi \\equiv W . \\Phi\n",
    "\\end{align}\n",
    "\n",
    "For example, consider this cross correlation:\n",
    "\n",
    "\\begin{align}\n",
    "w \\star \\phi = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\star\n",
    "\\begin{pmatrix}\n",
    "\\phi_{00} & \\phi_{01} & \\phi_{02} & \\phi_{03} \\\\\n",
    "\\phi_{10} & \\phi_{11} & \\phi_{12} & \\phi_{13} \\\\\n",
    "\\phi_{20} & \\phi_{21} & \\phi_{22} & \\phi_{23} \\\\\n",
    "\\phi_{30} & \\phi_{31} & \\phi_{32} & \\phi_{33}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Each element of $(w \\star \\phi)(x)$ can be seen as a dot-product between the vectorised kernel and configuration, after padding the kernel with zeros to match the size of $\\phi$ and rolling it so that the kernel is centred on $x$.\n",
    "\n",
    "For example, for the $(0, 0)$ element we first expand the kernel to match the size of $\\phi$ and roll it so that the (0, 0) element is in the top-left corner:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix} \\longrightarrow\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "and then flatten both kernel and input state\n",
    "\n",
    "\\begin{align}\n",
    "(w \\star \\phi)(0, 0) = \\begin{pmatrix}\n",
    "1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "\\phi_{00} \\\\ \\phi_{01} \\\\ \\phi_{02} \\\\ \\phi_{03} \\\\\n",
    "\\phi_{10} \\\\ \\phi_{11} \\\\ \\phi_{12} \\\\ \\phi_{13} \\\\\n",
    "\\phi_{20} \\\\ \\phi_{21} \\\\ \\phi_{22} \\\\ \\phi_{23} \\\\\n",
    "\\phi_{30} \\\\ \\phi_{31} \\\\ \\phi_{32} \\\\ \\phi_{33}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "For the $(1, 1)$ element we would start from\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix} \\longrightarrow\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{pmatrix} \\longrightarrow\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "and so on, until we have a $16 \\times 16$ matrix acting on the vector $\\Phi$.\n",
    "\n",
    "**Note:** The usefulness of this form is that we can (naively) compute the Jacobian of $W$ and hence the change in density due to the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115d7c69-9aec-401b-aa23-6097b31cd60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbecd686910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_matrix_vector(w, phi):\n",
    "    \"\"\"Convert a kernel and 2d state to an equivalent matrix-vector form.\n",
    "\n",
    "        w * \\phi  == W . Phi\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    w: 2-dimensional kernel\n",
    "    phi: 2-dimensional state, with optional batch dimension\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W: square matrix whose dimensions are equal to the size of a 2-d state\n",
    "    Phi: vectorised state, i.e. with lattice dimensions flattened\n",
    "    \"\"\"\n",
    "    assert phi.dim() in (2, 3)\n",
    "    if phi.dim() == 2:\n",
    "        phi = phi.unsqueeze(0)\n",
    "\n",
    "    # (0, 0) element is in top left\n",
    "    w = pad_and_roll_kernel(phi, w)\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    W = torch.stack(\n",
    "        [\n",
    "            w.roll((x1, x2), dims=(0, 1)).flatten()\n",
    "            for x1, x2 in itertools.product(range(w.shape[0]), range(w.shape[1]))\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "    Phi = phi.flatten(start_dim=1)\n",
    "\n",
    "    return W, Phi\n",
    "\n",
    "\n",
    "# ----------------------- #\n",
    "# TEST 1: Identity kernel #\n",
    "# ----------------------- #\n",
    "L = 4\n",
    "phi = torch.arange(L * L).view(L, L)\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ").int()\n",
    "W, Phi = to_matrix_vector(w, phi)\n",
    "assert torch.equal(W, torch.diag(torch.ones(Phi.shape[1]).int()))\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# TEST 2: Agreement with conv2d #\n",
    "# ----------------------------- #\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "W, Phi = to_matrix_vector(w, phi)\n",
    "mv_out = torch.mv(W, Phi.squeeze())\n",
    "assert torch.allclose(conv_out, mv_out.view(L, L))\n",
    "\n",
    "\n",
    "# --------------------- #\n",
    "# PLOT: Compare w and W #\n",
    "# --------------------- #\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    ").int()\n",
    "W, Phi = to_matrix_vector(w, phi)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title(\"Kernel\")\n",
    "ax1.imshow(w)\n",
    "ax2.set_title(\"Equivalent kernel as Matrix\")\n",
    "ax2.imshow(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a4141-f052-4682-8edc-f73ba6470abc",
   "metadata": {},
   "source": [
    "## Block-circulant matrix representation\n",
    "\n",
    "A **Toeplitz matrix** is a matrix with constant diagonals. The determinant of a Toeplitz matrix can be computed in $O(n^2)$ time using [Levinson recursion](https://en.wikipedia.org/wiki/Levinson_recursion).\n",
    "\n",
    "A **circulant matrix** is a special case of Toeplitz matrix where the diagonals 'wrap around' such that each row is equal to the row above, shifted one to the right. Circulant matrices are diagonalised by the DFT, which can be computed in $O(n\\log n)$ time using the FFT algorithm. The determinant can hence be computed in $O(n\\log n)$.\n",
    "\n",
    "Circulant matrices arise as kernels of one-dimensional convolution or cross-correlation acting upon the cyclic group $C_n$. Hence, one-dimensional cross-correlation applied to a one-dimensional lattice field theory would take the form\n",
    "\n",
    "\\begin{align}\n",
    "W_c^{1d} = \\begin{pmatrix}\n",
    "w_0 & w_1 &  \\ldots & w_L \\\\\n",
    "w_L & w_0 & \\ldots & w_{L-1}  \\\\\n",
    "\\vdots &  & \\ddots & \\vdots \\\\\n",
    "w_1 & w_2 & \\ldots & w_0\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In our case, we are interested two-dimensional cross-correlation acting on $C_n \\times C_n$ where $n = L$ the number of lattice sites in each dimension. This slightly changes things; in the matrix-vector form described above the matrix $W$ is not circulant, but **block circulant**.\n",
    "\n",
    "I doubt this will come in useful, but maybe if I had a 2d convolution kernel as the outputs of a coupling layer where the partitioning was over vector dimensions of the field, I might want to optimise the determinant computation... Anyway, see [this reference](https://mro.massey.ac.nz/bitstream/handle/10179/4456/Eigenvectors_of_block_circulant_and_alternating_circulant_matrices.pdf;sequence=1) if it becomes interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319c258-ccd2-40ce-bec8-4eb2b7963f0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rubbish bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccda14-c429-412b-936b-1aeef829ee73",
   "metadata": {},
   "source": [
    "Example: Free Theory\n",
    "\n",
    "Now consider the kernel\n",
    "\n",
    "$$\n",
    "w = \n",
    "\\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "b & d & e \\\\\n",
    "c & e & f\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "acting on a $4 \\times 4$ diagonal Gaussian\n",
    "\n",
    "$$\n",
    "\\phi = \n",
    "\\begin{pmatrix}\n",
    "\\phi_{00} & \\phi_{01} & \\phi_{02} & \\phi_{03} \\\\\n",
    "\\phi_{10} & \\phi_{11} & \\phi_{12} & \\phi_{13} \\\\\n",
    "\\phi_{20} & \\phi_{21} & \\phi_{22} & \\phi_{23} \\\\\n",
    "\\phi_{30} & \\phi_{31} & \\phi_{32} & \\phi_{33}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For example,\n",
    "\n",
    "$$\n",
    "(w \\star \\phi)_{00} = a \\phi_{33} + b(\\phi_{03} + \\phi_{30}) + c(\\phi_{13} + \\phi_{31}) + d\\phi_{00} + e(\\phi_{01} + \\phi_{10}) + f\\phi_{11}\n",
    "$$\n",
    "\n",
    "The diagonal elements of the covariance matrix are\n",
    "\n",
    "$$\n",
    "\\langle (w \\star \\phi)_{ii} (w \\star \\phi)_{ii} \\rangle \n",
    "= a^2 + 4b^2 + 4c^2 + d^2 + 4e^2 + f^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle (w \\star \\phi)_{00} (w \\star \\phi)_{01} \\rangle \n",
    "&= \\left\\langle \n",
    "\\left( a \\phi_{33} + b(\\phi_{03} + \\phi_{30}) + c(\\phi_{13} + \\phi_{31}) + d\\phi_{00} + e(\\phi_{01} + \\phi_{10}) + f\\phi_{11} \\right)\n",
    "\\left( a \\phi_{30} + b(\\phi_{31} + \\phi_{00}) + c(\\phi_{32} + \\phi_{10}) + d\\phi_{01} + e(\\phi_{02} + \\phi_{11}) + f\\phi_{12} \\right) \n",
    "\\right\\rangle \\\\\n",
    "&= ab \\langle\\phi_{30}^2\\rangle + cb \\langle\\phi_{31}^2\\rangle + db\\langle\\phi_{00}^2\\rangle + ed\\langle\\phi_{01}^2\\rangle + ec\\langle\\phi_{10}\\rangle + fe\\langle\\phi_{11}^2\\rangle \\\\\n",
    "&= ab + cb + db + ed + fe\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ff56e-41a4-4c6e-ae7d-9daa140629d8",
   "metadata": {},
   "source": [
    "First, expand the kernel to match the size of $\\phi$ and roll it so that the (0, 0) element is in the top-left corner:\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix} \\longrightarrow\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 0 & 1 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Usually we want a convolutional layer to encode some local structure in the data.\n",
    "This leads to kernels that are nonzero only within some radius $K$ of $y=(0, 0)$.\n",
    "For example, on a square lattice, a kernel with radius $K=1$ is represented by the following matrix\n",
    "\n",
    "\\begin{align}\n",
    "w(y) = \\begin{pmatrix}\n",
    "\\ddots \\\\\n",
    "& 0 & 0 & 0 & 0 & 0 \\\\\n",
    "& 0 & 0 & w(L, 0) & 0 & 0 \\\\\n",
    "& 0 & w(0, L) & w(0, 0) & w(0, 1) & 0 \\\\\n",
    "& 0 & 0 & w(1, 0) & 0 & 0 \\\\\n",
    "& 0 & 0 & 0 & 0 & 0 & \\\\\n",
    "&&&&&&\\ddots\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Discrete convolution or cross correlation can be expressed as a matrix-vector product\n",
    "\n",
    "$$\n",
    "w \\star \\phi \\equiv W . \\Phi\n",
    "$$\n",
    "\n",
    "where the vector $\\Phi$ is the flattened $\\phi$ and the matrix $W$ is a representation of $w$ as a **Toeplitz matrix** (a matrix with constant diagonals).\n",
    "For brevity, let $w(y_1, y_2)$ be written $w_{y_1y_2}$.\n",
    "The $K=1$ kernel described above now becomes\n",
    "\n",
    "\\begin{align}\n",
    "    W = \\begin{pmatrix}\n",
    "    w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots & 0 & w_{LL} \\\\\n",
    "    w_{LL} & w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} &  0 & \\ldots & 0\\\\\n",
    "    0 & w_{LL} & w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots \\\\\n",
    "    & & & & & \\ddots\\\\\n",
    "    w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots & 0 & w_{LL} & w_{00}\n",
    "    \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b67e5af-0bf3-49b3-a9c1-7e8df8583925",
   "metadata": {},
   "source": [
    "L = 6\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0.5],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "transform = ConvTransform(w)\n",
    "\n",
    "prior = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.zeros(L * L),\n",
    "    covariance_matrix=torch.block_diag(torch.diag(torch.ones(L * L))),\n",
    ")\n",
    "\n",
    "diag = torch.diag(torch.ones(L))\n",
    "block = diag.mul(1.25).add(diag.div(2).roll(1, 0)).add(diag.div(2).roll(1, 1))\n",
    "target_cov = torch.block_diag(*[block for _ in range(L)])\n",
    "print(torch.slogdet(target_cov))\n",
    "\n",
    "target = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.zeros(L * L), covariance_matrix=target_cov\n",
    ")\n",
    "\n",
    "transformed = torch.distributions.TransformedDistribution(prior, transform)\n",
    "\n",
    "transformed.sample()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e291c0f-a64d-4ae9-8e3e-bb0569c3fb6c",
   "metadata": {},
   "source": [
    "Bin\n",
    "\n",
    "Let's take again the example of \n",
    "\n",
    "\\begin{align}\n",
    "w \\star \\phi = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\star\n",
    "\\begin{pmatrix}\n",
    "\\phi_{00} & \\phi_{01} & \\phi_{02} & \\phi_{03} \\\\\n",
    "\\phi_{10} & \\phi_{11} & \\phi_{12} & \\phi_{13} \\\\\n",
    "\\phi_{20} & \\phi_{21} & \\phi_{22} & \\phi_{23} \\\\\n",
    "\\phi_{30} & \\phi_{31} & \\phi_{32} & \\phi_{33}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "We first expand the state $\\phi$ using periodic padding of size $k=1$ on each boundary:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "\\phi_{00} & \\phi_{01} & \\phi_{02} & \\phi_{03} \\\\\n",
    "\\phi_{10} & \\phi_{11} & \\phi_{12} & \\phi_{13} \\\\\n",
    "\\phi_{20} & \\phi_{21} & \\phi_{22} & \\phi_{23} \\\\\n",
    "\\phi_{30} & \\phi_{31} & \\phi_{32} & \\phi_{33}\n",
    "\\end{pmatrix}\n",
    "\\longrightarrow\n",
    "\\begin{pmatrix}\n",
    "\\phi_{33} & \\phi_{30} & \\phi_{31} & \\phi_{32} & \\phi_{33} & \\phi_{30} \\\\\n",
    "\\phi_{03} & \\phi_{00} & \\phi_{01} & \\phi_{02} & \\phi_{03} & \\phi_{00} \\\\\n",
    "\\phi_{13} & \\phi_{10} & \\phi_{11} & \\phi_{12} & \\phi_{13} & \\phi_{10} \\\\\n",
    "\\phi_{23} & \\phi_{20} & \\phi_{21} & \\phi_{22} & \\phi_{23} & \\phi_{20} \\\\\n",
    "\\phi_{33} & \\phi_{30} & \\phi_{31} & \\phi_{32} & \\phi_{33} & \\phi_{30} \\\\\n",
    "\\phi_{03} & \\phi_{00} & \\phi_{01} & \\phi_{02} & \\phi_{03} & \\phi_{00}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Once we vectorise this state we get \n",
    "\n",
    "\\begin{align}\n",
    "\\Phi_c = \\begin{pmatrix}\n",
    "\\phi_{33} \\\\ \\phi_{30} \\\\ \\vdots \\\\ \\phi_{03} \\\\ \\phi_{00}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Okay, now consider $w$ acting on the top-right element of the original state, i.e. centred on $\\phi_{00}$. As a dot-product this looks like\n",
    "\n",
    "\\begin{align}\n",
    "(W_c \\cdot \\Phi_c)(0) = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & \\ldots & 0 & 1 & 1 & 1 & 0 & \\ldots & 0 & 1 & 0 & \\ldots\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "\\phi_{33} \\\\ \\phi_{30} \\\\ \\phi_{31} \\\\ \\vdots \\\\ \\phi_{30} \\\\ \\phi_{03} \\\\ \\phi_{00} \\\\ \\phi_{01} \\\\ \\phi_{02} \\\\ \\vdots \\\\ \\phi_{13} \\\\ \\phi_{10} \\\\ \\phi_{11} \\\\ \\vdots\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Moving one to the right, so the kernel is centred on $\\phi_{01}$, we get the same row-vector, but rolled one unit right-wards."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2c12be0-a487-4695-823f-44d84b7918ae",
   "metadata": {},
   "source": [
    "def to_circulant(phi, w):\n",
    "    \"\"\"\n",
    "    Express 2d cross-correlation as a matrix-vector product, where\n",
    "    the matrix is a circulant matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # pad phi\n",
    "    phi, _ = conv2d_inputs(phi, w)\n",
    "    phi.squeeze_(1)\n",
    "    assert phi.dim() == 3\n",
    "\n",
    "    # pad and roll w\n",
    "    w = pad_and_roll_kernel(phi, w)\n",
    "    \n",
    "#     #####################\n",
    "    \n",
    "#     L1, L2 = phi.shape[-2:]\n",
    "#     k1, k2 = [(k - 1) // 2 for k in w.shape]\n",
    "\n",
    "#     # Pad with zeros to make it the same size as phi\n",
    "#     w = torch.nn.functional.pad(\n",
    "#         w, (0, L2 - w.shape[1], 0, L1 - w.shape[0]), mode=\"constant\", value=0\n",
    "#     )\n",
    "\n",
    "#     # Roll the kernel\n",
    "#     w = w.roll((-k1, -k2), (0, 1))\n",
    "\n",
    "#         #####################\n",
    "\n",
    "    phi = phi.flatten(start_dim=1)  # want a vector\n",
    "    \n",
    "    print(w.shape, phi.shape)\n",
    "    #wc = torch.stack([w.flatten().roll(i, dims=0) for i in range(phi.shape[1])], dim=0)\n",
    "    wc = torch.stack(\n",
    "        [\n",
    "            w.roll((x1, x2), dims=(0, 1)).flatten()\n",
    "            for x1, x2 in itertools.product(range(w.shape[0]), range(w.shape[1]))\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "    return phi, wc\n",
    "\n",
    "\n",
    "# ----------------------- #\n",
    "# TEST 1: identity kernel #\n",
    "# ----------------------- #\n",
    "L = 6\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ").int()\n",
    "phic, wc = to_circulant(phi, w)\n",
    "assert torch.equal(wc, torch.diag(torch.ones(phic.shape[1]).int()))\n",
    "\n",
    "# -------------------------- #\n",
    "# TEST 2: matrix is Toeplitz #\n",
    "# -------------------------- #\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "phic, wc = to_circulant(phi, w)\n",
    "assert all(\n",
    "    [\n",
    "        torch.allclose(torch.diag(wc, diagonal=i), wc[0, i])\n",
    "        for i in range(wc.shape[0] // 2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --------------------------- #\n",
    "# TEST 3: matrix is circulant #\n",
    "# --------------------------- #\n",
    "# matrix rolled one up == matrix rolled one right, is equivalent to\n",
    "# matrix row below == row rolled one right\n",
    "assert torch.allclose(wc.roll(-1, 1), wc.roll(1, 0))\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# TEST 4: agreement with conv2d #\n",
    "# ----------------------------- #\n",
    "L = 4\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 1],\n",
    "        [0, 2, 1],\n",
    "        [0, 1, 1],\n",
    "    ]\n",
    ")\n",
    "# w.add_(torch.rand_like(w))\n",
    "\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "pad = (w.shape[0] - 1) // 2  # padding in each dim\n",
    "Lpad = L + 2 * pad\n",
    "phic, wc = to_circulant(phi, w)\n",
    "mv_out = torch.mv(wc, phic.squeeze()).view(Lpad, Lpad)\n",
    "print(conv_out.squeeze())\n",
    "print(mv_out[pad:-pad, pad:-pad])\n",
    "assert torch.allclose(conv_out.squeeze(), mv_out[pad:-pad, pad:-pad])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9313c3e-1ebe-4b35-9286-5d05e68a07c3",
   "metadata": {},
   "source": [
    "L = 4\n",
    "phi = torch.arange(L * L).reshape(L, L).float()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "print(\"Output of conv2d:\")\n",
    "print(conv_out.squeeze())\n",
    "\n",
    "pad = (w.shape[0] - 1) // 2  # padding in each dim\n",
    "Lpad = L + 2 * pad\n",
    "phic, wc = to_circulant(phi, w)\n",
    "print(\"Circulant matrix\")\n",
    "print(wc[:10, :18].int())\n",
    "\n",
    "assert phic.shape == torch.Size([1, Lpad * Lpad])\n",
    "assert wc.shape == torch.Size([Lpad * Lpad, Lpad * Lpad])\n",
    "\n",
    "print(\"Phi after padding:\")\n",
    "print(phic.view(Lpad, Lpad))\n",
    "\n",
    "mv_out = torch.mv(wc, phic.squeeze()).view(Lpad, Lpad)\n",
    "\n",
    "print(\"Output of Wc . Phic, including padding:\")\n",
    "print(mv_out)\n",
    "\n",
    "assert torch.allclose(conv_out.squeeze(), mv_out[pad:-pad, pad:-pad])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
