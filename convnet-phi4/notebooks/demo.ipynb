{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8838a0-4dce-4804-9048-dabee2d835ba",
   "metadata": {},
   "source": [
    "# Convolution Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfada8c2-ef2c-4036-9811-54596893e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5121fe-94e9-418c-aa62-89838c4a07e4",
   "metadata": {},
   "source": [
    "## Definition of convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd63d4-3a20-4407-ba95-91a815997651",
   "metadata": {},
   "source": [
    "Let $\\phi(x)$ and $w(x)$ be real-valued scalar functions defined on $\\mathbb{R}^2$ which decay to zero as $x \\to \\pm \\infty$.\n",
    "Their convolution is another function defined by the integral transform,\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\ast \\phi)(x) = \\int_{\\mathbb{R}^2} \\mathrm{d} y \\, w(y) \\phi(x - y) \\, .\n",
    "\\end{align}\n",
    "\n",
    "We will take the view that $\\phi(x)$ is the function being transformed, and $w(y)$ is the **kernel** of the transform.\n",
    "The discretized version of this operation is\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\ast \\phi)(n) = \\sum_{m \\in \\mathbb{Z}^2} w(m) \\phi(n - m) \\, .\n",
    "\\end{align}\n",
    "\n",
    "We are interested in the situation where the domain is a square lattice $\\Lambda \\subset a \\mathbb{N}^2$, which is periodic in both dimensions with period $aL$, where $a$ is the lattice spacing.\n",
    "On this domain the convolution can be written\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\ast \\phi)(an) = \\sum_{m_1, m_2=0}^{L-1} w(am) \\phi\\big(a(n-m)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "Let us set the lattice spacing to $a = 1$ so that $x \\equiv n$ and $y \\equiv m$.\n",
    "\n",
    "\\begin{align} \\label{eq:conv_layer}\n",
    "    (w \\ast \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x-y)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "As it happens, the traditional conventional convolutional neural network performs a cross-correlation '$\\star$' rather than a convolution '$\\ast$'.\n",
    "For real-valued functions this differs by nothing more than a reflection in $\\phi$;\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\star \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x+y)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "Usually we want a convolutional layer to encode some local structure in the data.\n",
    "This leads to kernels that are nonzero only within some radius $K$ of $y=(0, 0)$.\n",
    "For example, on a square lattice, a kernel with radius $K=1$ is represented by the following matrix\n",
    "\n",
    "\\begin{align}\n",
    "w(y) = \\begin{pmatrix}\n",
    "\\ddots \\\\\n",
    "& 0 & 0 & 0 & 0 & 0 \\\\\n",
    "& 0 & 0 & w(L, 0) & 0 & 0 \\\\\n",
    "& 0 & w(0, L) & w(0, 0) & w(0, 1) & 0 \\\\\n",
    "& 0 & 0 & w(1, 0) & 0 & 0 \\\\\n",
    "& 0 & 0 & 0 & 0 & 0 & \\\\\n",
    "&&&&&&\\ddots\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Discrete convolution or cross correlation can be expressed as a matrix-vector product\n",
    "\n",
    "$$\n",
    "w \\star \\phi \\equiv W . \\Phi\n",
    "$$\n",
    "\n",
    "where the vector $\\Phi$ is the flattened $\\phi$ and the matrix $W$ is a representation of $w$ as a **Toeplitz matrix** (a matrix with constant diagonals).\n",
    "For brevity, let $w(y_1, y_2)$ be written $w_{y_1y_2}$.\n",
    "The $K=1$ kernel described above now becomes\n",
    "\n",
    "\\begin{align}\n",
    "    W = \\begin{pmatrix}\n",
    "    w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots & 0 & w_{LL} \\\\\n",
    "    w_{LL} & w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} &  0 & \\ldots & 0\\\\\n",
    "    0 & w_{LL} & w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots \\\\\n",
    "    & & & & & \\ddots\\\\\n",
    "    w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots & 0 & w_{LL} & w_{00}\n",
    "    \\end{pmatrix}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644020f2-e3b5-46ff-a0cc-0f3c93f076bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## PyTorch implementation: `conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc314bb-0567-4576-8797-33d15c85749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_inputs(phi, w):\n",
    "    \"\"\"Reshape and pad inputs for conv2d.\n",
    "    \n",
    "    conv2d expects shapes:\n",
    "    \n",
    "    - input (phi) : (batch size, input channels, width, height)\n",
    "    - kernel (w)  : (batch size, output channels, kernel width, kernel height)\n",
    "    \n",
    "    However, our input is periodic so we also need to pad phi by an amount\n",
    "    K, which is the kernel radius.\n",
    "    \"\"\"\n",
    "    assert phi.dim() == 2\n",
    "    assert w.dim() == 2\n",
    "    K1, K2 = [(k - 1) // 2 for k in w.shape]\n",
    "    phi = phi.view(1, 1, *phi.shape)\n",
    "    w = w.view(1, 1, *w.shape)\n",
    "    phi = torch.nn.functional.pad(\n",
    "        phi,  # (n_batch, n_channels, width, height)\n",
    "        pad=(K1, K1, K2, K2),  # pad last 2 dimensions by 1 on each side\n",
    "        mode=\"circular\",\n",
    "    )\n",
    "    return phi, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d89fbc-25a5-4059-aac2-781f1043a441",
   "metadata": {},
   "source": [
    "### Identity transformation\n",
    "\n",
    "Note that `conv2d` treats the matrix `w` as though it is centered on $(0, 0)$.\n",
    "Hence, the identity kernel should have a one in the middle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ad4e5-fd29-4e68-a3c8-51b99866266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 6\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.Tensor([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "assert torch.allclose(conv_out, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff389b68-890b-4257-836e-bfdc261e7ada",
   "metadata": {},
   "source": [
    "### Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9f3e2-14f9-4c54-beec-6f24710c1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "phi = torch.arange(L * L).reshape(L, L).float()\n",
    "w = torch.Tensor([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "])\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "# Check the direction of torch.roll\n",
    "assert phi.roll(shifts=(-1, -1), dims=(0, 1))[0, 0] == phi[1, 1]\n",
    "\n",
    "assert torch.allclose(conv_out, phi.roll(-1, dims=1))\n",
    "print(phi.int().numpy())\n",
    "print(conv_out.squeeze().int().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06ea89-2536-4bf5-8e98-49ff9da6ab01",
   "metadata": {},
   "source": [
    "### Verifying that `conv2d` implements cross-correlation\n",
    "\n",
    "I am trying to check that conv2d implements the cross-correlation calculation defined earlier in this notebook. This requires us to modify the kernel so that it looks like a full L1 x L2 matrix with the (0, 0) element in the top-left corner.\n",
    "    \n",
    "For example, conv2d interprets the following tensor as having 'b' in the (0, 0) position, 'a' in the (L, L) position and 'c' in the (1, 1) position:\n",
    "    \n",
    "```python\n",
    "torch.Tensor([\n",
    "    [a, 0, 0],\n",
    "    [0, b, 0],\n",
    "    [0, 0, c]\n",
    "])\n",
    "```\n",
    "However, to match our cross-correlation calculation we require an L1 x L2 tensor that looks like the following:\n",
    "    \n",
    "```python\n",
    "torch.Tensor([\n",
    "    [b, 0, ..., 0],\n",
    "    [0, c, ..., 0],\n",
    "         ...\n",
    "    [0, 0, ..., a]\n",
    "])\n",
    "```\n",
    "    \n",
    "Therefore, we need to:\n",
    "1. Pad the tensor with zeros to make it the same shape as the input\n",
    "2. Roll the kernel so that the (0, 0) element is in the top-left corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813574f7-3429-4176-be71-4b70a4263dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_roll_kernel(w, L1, L2):\n",
    "    \"\"\"Pad and roll kernel so that we can compute the cross-correlation.\n",
    "    \n",
    "    I am trying to check that conv2d implements the cross-correlation calculation\n",
    "    defined earlier in this notebook. This requires us to modify the kernel so\n",
    "    that it looks like a full L1 x L2 matrix with the (0, 0) element in the top-\n",
    "    left corner.\n",
    "    \n",
    "    For example, conv2d interprets the following tensor as having 'b' in the (0, 0)\n",
    "    position, 'a' in the (L, L) position and 'c' in the (1, 1) position:\n",
    "    \n",
    "    >>> [[a, 0, 0],\n",
    "         [0, b, 0],\n",
    "         [0, 0, c]]\n",
    "    \n",
    "    However, to match our cross-correlation calculation we require an L1 x L2 tensor\n",
    "    that looks like the following:\n",
    "    \n",
    "    >>> [[b, 0, ..., 0],\n",
    "         [0, c, ..., 0],\n",
    "         ...\n",
    "         [0, 0, ..., a]]\n",
    "    \n",
    "    Therefore, we need to:\n",
    "        \n",
    "        1) Pad the tensor with zeros to make it the same shape as the input\n",
    "        2) Roll the kernel so that the (0, 0) element is in the top-left corner\n",
    "    \"\"\"\n",
    "    K1, K2 = [(k - 1) // 2 for k in w.shape]\n",
    "    \n",
    "    # Pad with zeros to make it the same size as phi\n",
    "    w = torch.nn.functional.pad(w, (0, L1 - w.shape[0], 0, L2 - w.shape[1]), mode=\"constant\", value=0)\n",
    "    \n",
    "    # Roll the kernel\n",
    "    w = w.roll((-K1, -K2), (0, 1))\n",
    "    \n",
    "    return w\n",
    "\n",
    "L1, L2 = 6, 6\n",
    "w = torch.Tensor([\n",
    "    [1, 0, 0],\n",
    "    [0, 2, 0],\n",
    "    [0, 0, 3],\n",
    "]).int()\n",
    "expected = torch.Tensor([\n",
    "    [2, 0, 0, 0, 0, 0],\n",
    "    [0, 3, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1]]\n",
    ").int()\n",
    "result = pad_and_roll_kernel(w, L1, L2)\n",
    "assert torch.equal(result, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a7b6-db12-45fc-8d77-d3dce3752b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "w = pad_and_roll_kernel(w, L, L)\n",
    "\n",
    "cross_corr = torch.stack(\n",
    "    [\n",
    "        w[y1, y2] * phi.roll((-y1, -y2), (0, 1))\n",
    "        for y1, y2 in itertools.product(range(L), range(L))\n",
    "    ],\n",
    "    dim=0,\n",
    ").sum(dim=0)\n",
    "assert torch.allclose(cross_corr, conv_out)\n",
    "\n",
    "# To be explicit\n",
    "cross_corr_explicit = torch.zeros_like(phi)\n",
    "for x1 in range(L):\n",
    "    for x2 in range(L):\n",
    "        for y1 in range(L):\n",
    "            for y2 in range(L):\n",
    "                res = w[y1, y2] * phi[(x1 + y1) % L, (x2 + y2) % L]\n",
    "                cross_corr_explicit[x1, x2] += res\n",
    "\n",
    "assert torch.allclose(cross_corr_explicit, conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c12f9-f088-4d12-9a2f-fb50dc6760bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
