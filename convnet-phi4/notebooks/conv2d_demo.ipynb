{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8838a0-4dce-4804-9048-dabee2d835ba",
   "metadata": {},
   "source": [
    "# Convolution Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfada8c2-ef2c-4036-9811-54596893e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%load_ext lab_black\n",
    "\n",
    "PI = math.pi\n",
    "\n",
    "torch.set_printoptions(precision=3, sci_mode=False, threshold=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5121fe-94e9-418c-aa62-89838c4a07e4",
   "metadata": {},
   "source": [
    "## Definition of convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd63d4-3a20-4407-ba95-91a815997651",
   "metadata": {},
   "source": [
    "Let $\\phi(x)$ and $w(x)$ be real-valued scalar functions defined on $\\mathbb{R}^2$ which decay to zero as $x \\to \\pm \\infty$.\n",
    "Their convolution is another function defined by the integral transform,\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\ast \\phi)(x) = \\int_{\\mathbb{R}^2} \\mathrm{d} y \\, w(y) \\phi(x - y) \\, .\n",
    "\\end{align}\n",
    "\n",
    "We will take the view that $\\phi(x)$ is the function being transformed, and $w(y)$ is the **kernel** of the transform.\n",
    "The discretized version of this operation is\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\ast \\phi)(n) = \\sum_{m \\in \\mathbb{Z}^2} w(m) \\phi(n - m) \\, .\n",
    "\\end{align}\n",
    "\n",
    "We are interested in the situation where the domain is a square lattice $\\Lambda \\subset a \\mathbb{N}^2$, which is periodic in both dimensions with period $aL$, where $a$ is the lattice spacing.\n",
    "On this domain the convolution can be written\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\ast \\phi)(an) = \\sum_{m_1, m_2=0}^{L-1} w(am) \\phi\\big(a(n-m)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "Let us set the lattice spacing to $a = 1$ so that $x \\equiv n$ and $y \\equiv m$.\n",
    "\n",
    "\\begin{align} \\label{eq:conv_layer}\n",
    "    (w \\ast \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x-y)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "As it happens, the traditional conventional convolutional neural network performs a cross-correlation '$\\star$' rather than a convolution '$\\ast$'.\n",
    "For real-valued functions this differs by nothing more than a reflection in $\\phi$;\n",
    "\n",
    "\\begin{align}\n",
    "    (w \\star \\phi)(x) = \\sum_{y_1, y_2=0}^{L} w(y) \\phi\\big((x+y)_{\\text{mod} L}\\big) \\, .\n",
    "\\end{align}\n",
    "\n",
    "Usually we want a convolutional layer to encode some local structure in the data.\n",
    "This leads to kernels that are nonzero only within some radius $K$ of $y=(0, 0)$.\n",
    "For example, on a square lattice, a kernel with radius $K=1$ is represented by the following matrix\n",
    "\n",
    "\\begin{align}\n",
    "w(y) = \\begin{pmatrix}\n",
    "\\ddots \\\\\n",
    "& 0 & 0 & 0 & 0 & 0 \\\\\n",
    "& 0 & 0 & w(L, 0) & 0 & 0 \\\\\n",
    "& 0 & w(0, L) & w(0, 0) & w(0, 1) & 0 \\\\\n",
    "& 0 & 0 & w(1, 0) & 0 & 0 \\\\\n",
    "& 0 & 0 & 0 & 0 & 0 & \\\\\n",
    "&&&&&&\\ddots\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Discrete convolution or cross correlation can be expressed as a matrix-vector product\n",
    "\n",
    "$$\n",
    "w \\star \\phi \\equiv W . \\Phi\n",
    "$$\n",
    "\n",
    "where the vector $\\Phi$ is the flattened $\\phi$ and the matrix $W$ is a representation of $w$ as a **Toeplitz matrix** (a matrix with constant diagonals).\n",
    "For brevity, let $w(y_1, y_2)$ be written $w_{y_1y_2}$.\n",
    "The $K=1$ kernel described above now becomes\n",
    "\n",
    "\\begin{align}\n",
    "    W = \\begin{pmatrix}\n",
    "    w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots & 0 & w_{LL} \\\\\n",
    "    w_{LL} & w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} &  0 & \\ldots & 0\\\\\n",
    "    0 & w_{LL} & w_{00} & w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots \\\\\n",
    "    & & & & & \\ddots\\\\\n",
    "    w_{01} & 0 & \\ldots & 0 & w_{0L} & w_{10} & 0 & \\ldots & 0 & w_{LL} & w_{00}\n",
    "    \\end{pmatrix}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644020f2-e3b5-46ff-a0cc-0f3c93f076bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorch implementation: `conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc314bb-0567-4576-8797-33d15c85749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_inputs(phi, w):\n",
    "    \"\"\"Reshape and pad inputs for conv2d.\n",
    "\n",
    "    conv2d expects shapes:\n",
    "\n",
    "    - input (phi) : (batch size, input channels, width, height)\n",
    "    - kernel (w)  : (batch size, output channels, kernel width, kernel height)\n",
    "\n",
    "    However, our input is periodic so we also need to pad phi by an amount\n",
    "    K, which is the kernel radius.\n",
    "    \"\"\"\n",
    "    assert phi.dim() in (2, 3)\n",
    "    assert w.dim() == 2\n",
    "\n",
    "    if phi.dim() == 2:\n",
    "        phi = phi.unsqueeze(0)  # add batch dim\n",
    "    phi = phi.unsqueeze(1)  # add channel dim\n",
    "\n",
    "    K1, K2 = [(k - 1) // 2 for k in w.shape]\n",
    "    w = w.view(1, 1, *w.shape)\n",
    "    phi = torch.nn.functional.pad(\n",
    "        phi,  # (n_batch, n_channels, width, height)\n",
    "        pad=(K2, K2, K1, K1),  # pad last 2 dimensions by 1 on each side\n",
    "        mode=\"circular\",\n",
    "    )\n",
    "    return phi, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d89fbc-25a5-4059-aac2-781f1043a441",
   "metadata": {},
   "source": [
    "### Identity transformation\n",
    "\n",
    "Note that `conv2d` treats the matrix `w` as though it is centered on $(0, 0)$.\n",
    "Hence, the identity kernel should have a one in the middle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ad4e5-fd29-4e68-a3c8-51b99866266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 6\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "assert torch.allclose(conv_out, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff389b68-890b-4257-836e-bfdc261e7ada",
   "metadata": {},
   "source": [
    "### Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9f3e2-14f9-4c54-beec-6f24710c1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "phi = torch.arange(L * L).reshape(L, L).float()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "# Check the direction of torch.roll\n",
    "assert phi.roll(shifts=(-1, -1), dims=(0, 1))[0, 0] == phi[1, 1]\n",
    "\n",
    "assert torch.allclose(conv_out, phi.roll(-1, dims=1))\n",
    "print(phi.int().numpy())\n",
    "print(conv_out.squeeze().int().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06ea89-2536-4bf5-8e98-49ff9da6ab01",
   "metadata": {},
   "source": [
    "### Verifying that `conv2d` implements cross-correlation\n",
    "\n",
    "I am trying to check that conv2d implements the cross-correlation calculation defined earlier in this notebook. This requires us to modify the kernel so that it looks like a full L1 x L2 matrix with the (0, 0) element in the top-left corner.\n",
    "    \n",
    "For example, conv2d interprets the following tensor as having 'b' in the (0, 0) position, 'a' in the (L, L) position and 'c' in the (1, 1) position:\n",
    "    \n",
    "```python\n",
    "torch.Tensor([\n",
    "    [a, 0, 0],\n",
    "    [0, b, 0],\n",
    "    [0, 0, c]\n",
    "])\n",
    "```\n",
    "However, to match our cross-correlation calculation we require an L1 x L2 tensor that looks like the following:\n",
    "    \n",
    "```python\n",
    "torch.Tensor([\n",
    "    [b, 0, ..., 0],\n",
    "    [0, c, ..., 0],\n",
    "         ...\n",
    "    [0, 0, ..., a]\n",
    "])\n",
    "```\n",
    "    \n",
    "Therefore, we need to:\n",
    "1. Pad the tensor with zeros to make it the same shape as the input\n",
    "2. Roll the kernel so that the (0, 0) element is in the top-left corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813574f7-3429-4176-be71-4b70a4263dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_roll_kernel(phi, w):\n",
    "    \"\"\"Pad and roll kernel so that we can compute the cross-correlation.\n",
    "\n",
    "    I am trying to check that conv2d implements the cross-correlation calculation\n",
    "    defined earlier in this notebook. This requires us to modify the kernel so\n",
    "    that it looks like a full L1 x L2 matrix with the (0, 0) element in the top-\n",
    "    left corner.\n",
    "\n",
    "    For example, conv2d interprets the following tensor as having 'b' in the (0, 0)\n",
    "    position, 'a' in the (L, L) position and 'c' in the (1, 1) position:\n",
    "\n",
    "    >>> [[a, 0, 0],\n",
    "         [0, b, 0],\n",
    "         [0, 0, c]]\n",
    "\n",
    "    However, to match our cross-correlation calculation we require an L1 x L2 tensor\n",
    "    that looks like the following:\n",
    "\n",
    "    >>> [[b, 0, ..., 0],\n",
    "         [0, c, ..., 0],\n",
    "         ...\n",
    "         [0, 0, ..., a]]\n",
    "\n",
    "    Therefore, we need to:\n",
    "\n",
    "        1) Pad the tensor with zeros to make it the same shape as the input\n",
    "        2) Roll the kernel so that the (0, 0) element is in the top-left corner\n",
    "    \"\"\"\n",
    "    L1, L2 = phi.shape[-2:]\n",
    "    K1, K2 = [(k - 1) // 2 for k in w.shape]\n",
    "\n",
    "    # Pad with zeros to make it the same size as phi\n",
    "    w = torch.nn.functional.pad(\n",
    "        w, (0, L1 - w.shape[0], 0, L2 - w.shape[1]), mode=\"constant\", value=0\n",
    "    )\n",
    "\n",
    "    # Roll the kernel\n",
    "    w = w.roll((-K1, -K2), (0, 1))\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "L = 6\n",
    "phi = torch.empty(L, L)\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [0, 2, 0],\n",
    "        [0, 0, 3],\n",
    "    ]\n",
    ").int()\n",
    "expected = torch.Tensor(\n",
    "    [\n",
    "        [2, 0, 0, 0, 0, 0],\n",
    "        [0, 3, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "    ]\n",
    ").int()\n",
    "result = pad_and_roll_kernel(phi, w)\n",
    "assert torch.equal(result, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30a7b6-db12-45fc-8d77-d3dce3752b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "phi = torch.empty(L, L).normal_()\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "w = pad_and_roll_kernel(phi, w)\n",
    "\n",
    "cross_corr = torch.stack(\n",
    "    [\n",
    "        w[y1, y2] * phi.roll((-y1, -y2), (0, 1))\n",
    "        for y1, y2 in itertools.product(range(L), range(L))\n",
    "    ],\n",
    "    dim=0,\n",
    ").sum(dim=0)\n",
    "assert torch.allclose(cross_corr, conv_out)\n",
    "\n",
    "# To be explicit\n",
    "cross_corr_explicit = torch.zeros_like(phi)\n",
    "for x1 in range(L):\n",
    "    for x2 in range(L):\n",
    "        for y1 in range(L):\n",
    "            for y2 in range(L):\n",
    "                res = w[y1, y2] * phi[(x1 + y1) % L, (x2 + y2) % L]\n",
    "                cross_corr_explicit[x1, x2] += res\n",
    "\n",
    "assert torch.allclose(cross_corr_explicit, conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a4141-f052-4682-8edc-f73ba6470abc",
   "metadata": {},
   "source": [
    "### Verifying the Toeplitz representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0e59e-59f3-4629-b65b-ec2e8503f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_toeplitz(phi, w):\n",
    "\n",
    "    # pad phi\n",
    "    phi, _ = conv2d_inputs(phi, w)\n",
    "    phi.squeeze_(1)\n",
    "    assert phi.dim() == 3\n",
    "\n",
    "    # pad and roll w\n",
    "    w = pad_and_roll_kernel(phi, w)\n",
    "\n",
    "    phi = phi.flatten(start_dim=1)  # want a vector\n",
    "    t = torch.stack([w.flatten().roll(i, dims=0) for i in range(phi.shape[1])], dim=0)\n",
    "    return phi, t\n",
    "\n",
    "\n",
    "L = 4\n",
    "phi = torch.arange(L * L).view(L, L)\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ").int()\n",
    "phi_vec, t = to_toeplitz(phi, w)\n",
    "assert torch.equal(t, torch.diag_embed(torch.ones(phi_vec.shape[1]).int()))\n",
    "assert torch.logdet(t.float()) == 0.0\n",
    "\n",
    "w = torch.empty(L - 1, L - 1).uniform_()\n",
    "phi_vec, t = to_toeplitz(phi, w)\n",
    "assert all(\n",
    "    [torch.allclose(torch.diag(t, diagonal=i), t[0, i]) for i in range(t.shape[0] // 2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076818e-b70e-4bc0-9360-ec84c8cebc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "phi = torch.arange(L * L).reshape(L, L).float()\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 1],\n",
    "        [0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "print(\"Output of conv2d:\")\n",
    "print(conv_out.squeeze())\n",
    "\n",
    "pad = (w.shape[0] - 1) // 2  # padding in each dim\n",
    "Lpad = L + 2 * pad\n",
    "phi_vec, t = to_toeplitz(phi, w)\n",
    "\n",
    "assert phi_vec.shape == torch.Size([1, Lpad * Lpad])\n",
    "assert t.shape == torch.Size([Lpad * Lpad, Lpad * Lpad])\n",
    "\n",
    "print(\"Phi after padding:\")\n",
    "print(phi_vec.view(Lpad, Lpad))\n",
    "\n",
    "toeplitz_out = torch.mv(t, phi_vec.squeeze()).view(Lpad, Lpad)\n",
    "\n",
    "print(\"Output of W . Phi, including padding:\")\n",
    "print(toeplitz_out)\n",
    "\n",
    "assert torch.allclose(conv_out.squeeze(), toeplitz_out[pad:-pad, pad:-pad])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be157bfe-7794-4ff3-9670-65d8e6f40296",
   "metadata": {},
   "source": [
    "## Applying convolutions to a Gaussian with diagonal covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dabfaf-20aa-426d-98ee-407bfde7453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_length(covariance):\n",
    "    L = covariance.shape[0]\n",
    "    covariance = covariance.div(covariance.max())\n",
    "\n",
    "    kernel = torch.cos(2 * PI / L * torch.arange(L)).view(L, 1)\n",
    "\n",
    "    g_tilde_00 = float(covariance.sum())\n",
    "    g_tilde_10 = float(covariance.mul(kernel).sum())\n",
    "\n",
    "    xi_sq = (g_tilde_00 / g_tilde_10 - 1) / (4 * math.sin(PI / L) ** 2)\n",
    "\n",
    "    try:\n",
    "        return math.sqrt(xi_sq)\n",
    "    except ValueError as exc:\n",
    "        print(\"Unable to compute correlation length\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def plot_covariance(phi):\n",
    "    \"\"\"Computes and plots the covariance matrix for a sample.\"\"\"\n",
    "    assert phi.dim() == 3\n",
    "    cov = torch.cov(phi.flatten(start_dim=1).T)\n",
    "    assert cov.shape == torch.Size([phi.flatten(start_dim=1).shape[1]] * 2)\n",
    "\n",
    "    cov_with_geometry = torch.stack(\n",
    "        [\n",
    "            row.view(L, L).roll((-(i // L), -(i % L)), dims=(0, 1))\n",
    "            for i, row in enumerate(cov.split(1, dim=0))\n",
    "        ],\n",
    "        dim=0,\n",
    "    ).mean(dim=0)\n",
    "\n",
    "    xi = correlation_length(cov_with_geometry)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    ax1.set_title(\"Covariance of flattened configs\")\n",
    "    ax1.imshow(cov)\n",
    "    ax2.set_title(\"Covariance of 2d configs\")\n",
    "    ax2.imshow(cov_with_geometry.roll((L // 2 - 1, L // 2 - 1), dims=(0, 1)))\n",
    "    ax3.set_title(\"Covariance along one dimension\")\n",
    "    ax3.plot(cov_with_geometry.sum(dim=1), \"o-\")\n",
    "    ax3.set_yscale(\"log\")\n",
    "    ax3.annotate(\n",
    "        f\"Correlation length: {xi:.2g}\", xy=(0.2, 0.9), xycoords=\"axes fraction\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31bc87-a4b8-4b85-abfc-d34c8f166e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "prior = torch.distributions.Normal(loc=torch.zeros(L, L), scale=torch.ones(L, L))\n",
    "phi = prior.sample([10000])\n",
    "plot_covariance(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6926b5d-ac7d-4d3e-aa90-47e9608b49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 1],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w))\n",
    "\n",
    "plot_covariance(conv_out.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24712354-1498-4cef-b26d-c01026b75faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conv = 6\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "conv_out = phi\n",
    "\n",
    "for _ in range(n_conv):\n",
    "    conv_out = torch.nn.functional.conv2d(*conv2d_inputs(conv_out, w)).squeeze(1)\n",
    "plot_covariance(conv_out.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a0dd5d-1c9f-4168-8368-d57568329a99",
   "metadata": {},
   "source": [
    "## Computing the density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a908327-fe5d-4577-83a2-d1abd911dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "prior = torch.distributions.Normal(loc=torch.zeros(L, L), scale=torch.ones(L, L))\n",
    "phi = prior.sample([10000])\n",
    "log_prob_prior = prior.log_prob(phi).flatten(start_dim=1).sum(dim=1)\n",
    "\n",
    "w = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "conv_out = torch.nn.functional.conv2d(*conv2d_inputs(phi, w)).flatten(start_dim=1)\n",
    "loc = conv_out.mean(dim=0)\n",
    "cov = torch.cov(conv_out.T)\n",
    "\n",
    "dist = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=cov)\n",
    "log_prob_new = dist.log_prob(conv_out)\n",
    "\n",
    "phi_vec, t = to_toeplitz(phi, w)\n",
    "\n",
    "print(log_prob_prior[0], torch.logdet(t), log_prob_new[0])\n",
    "log_prob_prior - torch.logdet(t) - log_prob_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb94b92-de84-41e0-a836-c163de21e21b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
